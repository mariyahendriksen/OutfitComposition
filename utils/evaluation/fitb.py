import argparse

import pandas as pd


def compute_fitb(test_annotation_file: str, user_submission_file: str):
    """
    Evaluates the submission for a particular challenge phase and returns the FITB score
    Arguments:
        `test_annotations_file`: Path to test_annotation_file on the server
        `user_submission_file`: Path to file submitted by the user
    :return
        The FITB score for the
    """
    gt = pd.read_csv(test_annotation_file, )
    gt.head()
    predicted = pd.read_csv(user_submission_file)
    predicted.head()
    try:
        if predicted.outfit_id.nunique() != len(predicted):
            print(
                f"There are more rows than expected or repeated outfit ids in this submission "
                f"{predicted.outfit_id.nunique()} != {len(predicted)}"
            )
            return 0.
        df = predicted.merge(gt, on=["outfit_id"], how="inner")
        df["acc"] = df["missing_product"] == df["predicted_product"]

        return df["acc"].sum() / len(gt)
    except Exception as e:
        print(str(e))
    return 0.


if __name__ == "__main__":
    description = """
    Compute the FITB metric given a file containing the expected 'missing_product' and another one with
    the corresponding 'predicted_product' for each 'outfit_id'.
    """
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        "--predicted-product-file",
        required=True,
        help="Path for the missing products (generated by your algorithm)",
        dest="predicted_product"
    )
    parser.add_argument(
        "--missing-product-file",
        required=True,
        help="Path for the expected answer provided by the dataset",
        dest="missing_product"
    )
    args = parser.parse_args()
    print("FITB for", args.missing_product, args.predicted_product)
    print(compute_fitb(args.missing_product, args.predicted_product))
